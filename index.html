<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  </script>

  <title>Hossein kashiani</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="png" href="Doc/icon.png">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:6%;width:63%;text-align: justify">
              <p style="text-align:center">
                <name>Hossein Kashiani</name>
              </p>
			  <p>I am a research assistant at the Computer Vision Center at the Iran University of Science & Technology. I received my master's degree in Electrical Engineering in 2018, supervised by Prof. <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar Baradaran Shokouhi</a>. My long-term research goal is to enable AI agents to explain phenomena beyond low-level statistics of observable data.
              </p>

              </p>
              <p style="text-align:center">

				<a href="mailto:kashianihossein@gmail.com">Email</a> &nbsp/&nbsp
                <!--<a href="Doc/homepage_cv.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://www.linkedin.com/in/hossein-kashiani/">LinkedIn</a> &nbsp/&nbsp
     		<a href="https://github.com/kashiani">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
			  <img width=100% src="Doc/circle-cropped.png">

            </td>
           </tr>
</p>
        </tbody></table>
        <table style="width:100%"><tbody>
            <tr>
            <td style="width:100%">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='Doc/IMAVIS_1.png'></div>
                <img src='Doc/IMAVIS_1.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_image').style.opacity = "1";
                }
                function nightsight_stop() {
                  document.getElementById('nightsight_image').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle> Visual object tracking based on adaptive siamese and motion estimation network.</papertitle>
              </a>
              <br>
              <strong>Hossein Kashiani</strong>,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>Image and Vision Computing</em>, 2019

              <p>In this work, we aim to improve both motion and observation models in visual object tracking by leveraging the
 		representation power of CNNs. To this end, a motion estimation network is utilized to seek the most likely
 		locations of the target and prepare a further clue in addition to the previous position of the target.
 		As a result, the motion estimation is enhanced by generating a small number of candidates near two feasible positions.
 		The generated candidates are then fed into a trained Siamese network to detect the most probable candidate.
 		Each candidate is compared to an adaptable buffer, which is updated under a predefined condition.
 		To take into account target appearance changes, a weighting CNN adaptively assigns weights to the
 		final similarity scores of the Siamese network using sequence-specific information.</p>
            </td>
          </tr>
          
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='font_image'><img src='Doc/ICCKE_2.png'></div>
                <img src='Doc/ICCKE_2.png'>
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }
                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle>Patchwise object tracking via structural local sparse appearance model.</papertitle>
              </a>
              <br>
              <strong>Hossein Kashiani</strong>,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>International Conference on Computer and Knowledge Engineering </em>, 2017

              <p>In this paper, we propose a robust visual tracking method which exploits the relationships of targets
			  in adjacent frames using patchwise joint sparse representation. Two sets of overlapping patches with different
			  sizes are extracted from target candidates to construct two dictionaries with consideration of joint sparse representation.
			  By applying this representation into structural sparse appearance model, we can take two-fold advantages.
			  First, correlation of target patches is considered over time. Second, using this local appearance model with different
			  patch sizes takes into account local features of target thoroughly. Furthermore, the position of candidate patches and
			  their occlusion levels are utilized simultaneously to obtain the final likelihood of target candidates.</p>
            </td>
          </tr>
         </tr>
       </tbody></table>
        <table style="width:100%"><tbody>
            <tr>
            <td style="width:100%;">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%;margin-left:auto"><tbody>

          <tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/2.png'></div>
                <img src='Doc/1.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">

                <papertitle> Online Visual Tracking with One-Shot Context-Aware Domain Adaptation.</papertitle>
			  <br>
              <strong>Hossein Kashiani</strong>,
			   Amir Abbas H. Imani,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a>,
			  <a href="https://scholar.google.nl/citations?user=KjumZJgAAAAJ&hl=en&authuser=1">Ahmad Ayatollahi</a>

              <p> Online learning policy makes visual trackers more robust against different
				distortions through learning domain-specific cues. However, the trackers adopting this policy fail to fully leverage the discriminative context of the background
				areas. Moreover, owing to the lack of sufficient data at each time step, the online learning approach can also make the trackers prone to over-fitting to the
				background regions. In this paper, we propose a domain adaptation approach to
				strengthen the contributions of the semantic background context. The domain
				adaptation approach is backboned with only an off-the-shelf deep model. The
				strength of the proposed approach comes from its discriminative ability to handle severe occlusion and background clutter challenges.</p>
            </td>
          </tr>	  
	</tbody></table>
        <table style="width:100%;"><tbody>
            <tr>
            <td style="width:100%;">
              <heading>Projects</heading>
			  <br><br><br><br><br>
			  </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%"><tbody>

         <tr onmouseout="nightsight_stop_2()" onmouseover="nightsight_start_2()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_2'><img src='Doc/Generalizing.png'></div>
                <img src='Doc/Generalizing.png'>
              <script type="text/javascript">
                function nightsight_start_2() {
                  document.getElementById('nightsight_image_2').style.opacity = "1";
                }
                function nightsight_stop_2() {
                  document.getElementById('nightsight_image_2').style.opacity = "0";
                }
                nightsight_stop_2()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle> Generalizing State-of-the-art Object Detectors for Autonomous Vehicles in Unseen Environments.</papertitle>
              </a>


              <p> The ability to detect various objects in an unseen environment is a crucial yet
				tricky task for autonomous vehicles (AVs). Despite the significant improvements in recent years, researchers have yet to achieve an accurate perception
				of an unknown environment. In this respect, data-driven models are one of the bottlenecks for researches to enjoy advances in the object detection field.
				That is to say, due to the distribution mismatch, models trained on the available datasets fail to generalize well to the complex, real-world scenarios with
				higher dynamics. In this work, we investigate the generalization of one- and two-stage object detectors to our previously unseen dataset and attempt to handle the mentioned bottleneck. </p>
            </td>
			
         </tr>

		<table width="100%">
		<tr><td>
			<heading>Teaching Assistant</heading>
			<ul>
			<li> Computer Vision, Spring 2021</li>
			<li> Machine Learning, Fall 2020</li>
			<li> Computer Vision, Fall 2018</li>
			<li> Intelligent Systems, Fall 2017</li>
			</ul>
		  </td></tr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
		      Time saved by <a href="https://jonbarron.info//">this</a> awesome website.
	    </font>
        </p>
        </td>
        </tr>
		
      </td>
    </tr>
  </table>
</body>

</html>
