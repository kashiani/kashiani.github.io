<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FYEK75ZVZX"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-FYEK75ZVZX');
    </script>


  <title>Hossein Kashiani</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="png" href="Doc/icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <style>
      /* Add smooth scrolling */
      html {
        scroll-behavior: smooth;
      }

      /* Navigation Bar Styles */
      .navigation {
        background-color: #ffffff;
        overflow: hidden;
        position: fixed;
        top: 0;
        width: 100%;
        z-index: 1000;
        text-align: center;
        padding: 10px 0;
        border-bottom: 1px solid #ddd;
      }

      .navigation a {
        color: #1772d0;
        padding: 14px 16px;
        text-decoration: none;
        font-size: 15px; /* Reduced font size */
      }

      .navigation a:hover {
        background-color: #ddd;
        color: black;
      }

      /* Ensure z-index layering for content */
      * {
       z-index: 2;
      }

      /* White particle background */
       #particles-js {
         position: fixed;
         width: 100%;
         height: 100%;
         left: 0px;
         top: 0px;
         z-index: -1;
         background-color: #ffffff;
         pointer-events: auto; /* Ensures mouse events are captured */
       }
      .particle, .particle > canvas, #particles-js {
       z-index: -1 !important;
      }

    #center-fade {
      position: fixed;
      top: 0;
      left: 50%;
      transform: translateX(-50%);
      width: 100%;
      max-width: 800px; /* Same max-width as your main content */
      height: 100%;
      background-color: rgba(255, 255, 255, 0.92); /* Semi-transparent white */
      z-index: 1; /* Sits between particles (-1) and content (2) */
      pointer-events: none; /* Allows mouse clicks to pass through */
    }

       .news {
         margin-top: -5px; /* Reduce space before the News section */
          }

      /* Set body background color and base font size */
      body {
       background-color: #ffffff;
       padding-top: 50px; /* Add padding to prevent content from being hidden by the fixed navbar */
      }

      /* --- FONT SIZE OVERRIDE FOR ALL TEXT --- */
      body, p, li, td, strong, em, .font, a {
        font-size: 14px !important; /* Force smaller font size on all text elements */
      }

      /* Adjust custom tag font sizes */
      name {
        font-size: 26px !important; /* Further reduced heading size */
      }
      heading {
        font-size: 20px !important; /* Further reduced section heading size */
      }
      papertitle {
        font-size: 15px !important; /* Further reduced publication title size */
      }

      /* Keep navigation text size distinct */
      .navigation a {
        font-size: 15px !important;
      }


       @media only screen and (max-width: 768px) {
         td[style*="width:63%"], td[style*="width:40%"] {
          display: block;
          width: 100% !important;
          padding: 5% !important;
         }

         .news ul {
          height: auto !important;
         }

         img {
          width: 100% !important;
          height: auto !important;
         }

         #publications table {
          width: 100% !important;
          display: block;
          overflow-x: auto;
         }

         .image-container img {
          width: 100% !important;
          max-width: 100% !important;
          height: auto !important;
         }
       }

    </style>

</head>

<body>
  <div class="navigation">
    <a href="#news">News</a>
    <a href="#research">Research</a>
    <a href="#publications">Publications</a>
    <a href="#talks">Talks</a>
    <a href="#academic-service">Academic Service</a>
  </div>

  <div id="particles-js"></div>
  <div id="center-fade"></div>
  <div style="position: relative; margin-top: -30px;">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:3%; padding-top: 10px; width:63%;text-align: justify;vertical-align:top">
                  <p style="text-align:center">
                    <name>Hossein Kashiani</name>
                  </p>
                  <p>
                   I am a fifth-year Ph.D. candidate in the <a href='https://sites.google.com/g.clemson.edu/is-win-lab/home'>IS-WiN Lab</a> at <a href='https://www.clemson.edu/index.html'>Clemson University</a> (CU), advised by Dr. <a href='https://scholar.google.com/citations?hl=en&user=67mA71QAAAAJ&view_op=list_works&sortby=pubdate'>Fatemeh Afghah</a>. Before joining CU, I worked as a research assistant on biometrics at <a href="https://www.wvu.edu/">West Virginia University</a>. I completed my Master's degree in Electrical Engineering at <a href='https://www.iust.ac.ir/en'>Iran University of Science & Technology</a>. My research focuses on enhancing the generalization of machine learning models to unseen domains, with applications spanning various areas, including anomaly detection, biometrics, healthcare, visual perception tasks, and scene understanding.
                  </p>
                  <p style="text-align:center;margin-top:25px;margin-bottom:40px;padding-left:100px;">
                    <a href="mailto:kashianihossein@gmail.com" title="Email" style="margin:0 8px;"><i class="fa fa-envelope" style="font-size:24px"></i></a>
                    <a href="https://scholar.google.com/citations?hl=en&user=koUqDrgAAAAJ&view_op=list_works&sortby=pubdate" title="Google Scholar" style="margin:0 8px;"><img src="Doc/google-scholar.svg" alt="Google Scholar" style="width:24px;height:24px;vertical-align:middle;position:relative;top:-3px;filter:invert(36%) sepia(77%) saturate(1817%) hue-rotate(188deg) brightness(91%) contrast(88%);"></a>
                    <a href="https://github.com/kashiani" title="Github" style="margin:0 8px;"><i class="fa-brands fa-github" style="font-size:24px"></i></a>
                    <a href="https://www.linkedin.com/in/hossein-kashiani/" title="LinkedIn" style="margin:0 8px;"><i class="fa-brands fa-linkedin" style="font-size:24px"></i></a>
                    <a href="https://x.com/Hossein_serein" title="Twitter" style="margin:0 8px;"><i class="fa-brands fa-twitter" style="font-size:24px"></i></a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%;vertical-align:top">
                  <img width=100% src="Doc/circle-cropped.png">
                </td>
              </tr>
            </tbody>
          </table>

          <table id="news" style="margin-top: -50px; width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

            <tbody>
              <tr>
              <tr>
                <td style="padding:0px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <div class="news">
                    <ul style="text-align:justify;height: 150px;overflow-y: auto;">
          <li class="font">[2026/01] One paper on VLM-based anomaly detection has been accepted to ICASSP 2026.</li>
          <li class="font">[2025/05] One paper on Vision-Language Models has been accepted at KDD 2025.</li>
          <li class="font">[2025/02] FreqDebias has been accepted at CVPR 2025. </li>
          <li class="font">[2024/11] Two papers have been accepted at WACV 2025. </li>
             <li class="font">[2024/11] Successfully passed Qualifying Exam. </li>
                      <li class="font"><font color="black">[2024/06] My new paper <a href="https://arxiv.org/pdf/2401.03037" target="_blank">CATFace</a> is accepted by IEEE Transactions on Biometrics, Behavior, and Identity Science.</font></li>
                      <li class="font">[2024/02] AAFACE is accepted at IEEE ICIP 2023.</li>
                      <li class="font">[2023/11] <a href="https://github.com/Omid-Nejati/MedViT" target="_blank">MedViT</a> has been featured in Computers in Biology and Medicine.</li>
                      <li class="font">[2023/09] Our new method on Morph Attack Detection has been accepted by IJCB 2023.</li>
                  <li class="font">[2023/03] Our <a href="https://arxiv.org/pdf/2209.08130" target="_blank">face morphing detector</a> ranks among the top in NIST's Face Recognition Vendor Test (<a href="https://pages.nist.gov/frvt/html/frvt_morph.html" target="_blank">FRVT</a>).</li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


<table id="research" class="content-table" style="margin-top: 0; padding-top: 0;">
  <tbody>
    <tr>
      <td style="padding:0px;width:100%;vertical-align:middle; text-align: center;">
        <heading style="text-align: left; display: block;">Research</heading>
          <p style="text-align: justify;">
          My current research focuses on advancing Multimodal Large Language Models to enhance the robustness and adaptability of anomaly detection in few-shot settings and provide detailed descriptions of each detected irregularity. My prior work at CU includes developing advanced prompting techniques for Vision-Language models and robust multi-class anomaly detection.
          </p>
          <div style="text-align: center; margin: 15px 0;">
            <a href="#" onclick="toggleResearchDetails(); return false;" style="color: #1772d0; text-decoration: none; border: 1px solid #1772d0; padding: 6px 16px; border-radius: 4px; display: inline-block; font-size: 14px;">Read More â†“</a>
          </div>
          <div id="research_details" style="display:none;">
          <p style="text-align: justify;">
          At WVU, in collaboration with CITER and NIST, I explored advanced security measures for automated face recognition systems, focusing on robust <a href="https://github.com/kashiani/Face-Morphing-Attack-Detection-Benchmark" target="_blank">detection </a> and <a href="https://github.com/kashiani/Morph-Attack-Synthesis" target="_blank">generation </a> of face morphing attacks. Additionally, I worked on multimodal biometric recognition under long-range constraints to address issues related to turbulent, low-quality imagery from extended distances.
          </p>
          <p style="text-align: justify;">
          I also engaged in advancing Vision Transformers for healthcare at IUST. This includes developing  <a href="https://github.com/Omid-Nejati/MedViT" target="_blank">hybrid Transformers</a>  with locality inductive biases for medical imaging; enhancing adversarial robustness with innovative data augmentation techniques like <a href="https://github.com/Omid-Nejati/Locality-iN-Locality" target="_blank">Moment Exchanger and Patch Momentum Changer</a>; improving local and global dependencies within Transformer architectures.
          </p>
          <p style="text-align: justify;">
          Beyond these areas, my research extends to visual perception and scene understanding, such as object <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300113" target="_blank">tracking</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417421008368" target="_blank">detection</a>, and <a href="https://ieeexplore.ieee.org/iel7/6287639/9312710/09522137.pdf" target="_blank">segmentation</a>. This includes enhancing the robustness, generalization, and adaptability of perception models such as object detection and semantic segmentation for autonomous vehicles; developing multi-teacher knowledge distillation framework to enhance the performance of lightweight perception models, ensuring they operate effectively in challenging environments.
          </p>
          </div>
        <div id="scholar-metrics" style="background-color: #f0f0f0; border-radius: 10px; padding: 10px; margin: auto; display: inline-block;">
          <span style="color: #1772d0;">Citations: <span id="citation-count" style="color: #1772d0;">0</span> | H-Index: <span id="h-index" style="color: #1772d0;">0</span> | i10-Index: <span id="i10-index" style="color: #1772d0;">0</span></span>
        </div>

        <script>
         async function loadScholarData() {
            const response = await fetch("scholar_data.json");
            const data = await response.json();

            const duration = 2000;
            const startTime = new Date().getTime();

            function countUp() {
               const currentTime = new Date().getTime();
               const elapsed = currentTime - startTime;
               const progress = Math.min(elapsed / duration, 1);

               document.getElementById("citation-count").innerText = Math.round(data.citations * progress);
               document.getElementById("h-index").innerText = Math.round(data.h_index * progress);
               document.getElementById("i10-index").innerText = Math.round(data.i10_index * progress);

               if (progress < 1) {
                  requestAnimationFrame(countUp);
               }
            }

            countUp();
         }

         loadScholarData();
         </script>
      </td>
    </tr>
  </tbody>
</table>




          <div id="publications">
            <table style="width:100%">
              <tbody>
                <tr>
                  <td style="width:100%">
                    <heading>Publications</heading>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%">
              <tbody>

			<tr onmouseout="tcd_stop_promptmad()" onmouseover="tcd_start_promptmad()">
				<td style="padding:10px;width:25%;vertical-align:top">
					<div class="image-container">
						<img id="paper_promptmad" src="Doc/PROMPTMAD.png" alt="Image of PromptMAD Paper" width="200" style="border-style: none">
					</div>

					<script type="text/javascript">
						// Function to change the image when hovered (PromptMAD)
						function tcd_start_promptmad() {
							document.getElementById('paper_promptmad').src = "Doc/PROMPTMAD.png";
						}

						// Function to revert back to the original image (PromptMAD)
						function tcd_stop_promptmad() {
							document.getElementById('paper_promptmad').src = "Doc/PROMPTMAD.png";
						}
					</script>
				</td>
				<td style="padding-bottom:35px;width:75%;vertical-align:middle">
					<a href="https://arxiv.org/abs/" id="PromptMAD_journal">
						<papertitle style="color:#1772d0;">PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization</papertitle>
					</a>
					<br>
					<strong>Duncan McCain, Hossein Kashiani, Fatemeh Afghah</strong>
					<br>
					<em>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2026.
					<br>
					<a href="https://arxiv.org/abs/2601.22492">arxiv</a> /
					<a href="Doc/PromptMAD.bib">BibTeX</a> /
					<a href="#" onclick="toggleSummary('promptmad'); return false;">Summary</a>
					<p id="promptmad_summary" align="justify" style="display:none;">
						We propose PromptMAD, a cross-modal prompting framework for multi-class visual anomaly localization that leverages vision-language models to improve anomaly detection and localization across multiple classes.
					</p>
				</td>
			</tr>

			<tr>
				<td style="padding:10px;width:25%;vertical-align:top">
					<img src="Doc/FreqDebias2.png" alt="FreqDebias Paper" width="200" style="border-style: none">
				</td>
				<td style="padding-bottom:35px;width:75%;vertical-align:middle">
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kashiani_FreqDebias_Towards_Generalizable_Deepfake_Detection_via_Consistency-Driven_Frequency_Debiasing_CVPR_2025_paper.html" id="FreqDebias_journal">
						<papertitle style="color:#1772d0;">FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing</papertitle>
					</a>
					<br>
					<strong>Hossein Kashiani, Niloufar Alipour Talemi, Fatemeh Afghah</strong>
					<br>
					<em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.
					<br>
					<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Kashiani_FreqDebias_Towards_Generalizable_Deepfake_Detection_via_Consistency-Driven_Frequency_Debiasing_CVPR_2025_paper.html">CVF</a> /
					<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Kashiani_FreqDebias_Towards_Generalizable_Deepfake_Detection_via_Consistency-Driven_Frequency_Debiasing_CVPR_2025_paper.pdf">PDF</a> /
					<a href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Kashiani_FreqDebias_Towards_Generalizable_CVPR_2025_supplemental.pdf">Suppl</a> /
					<a href="https://www.youtube.com/watch?v=SHThZg1zcac">Video</a> /
					<a href="https://cvpr.thecvf.com/virtual/2025/poster/33990">Poster</a> /
					<a href=Doc/FreqDebias.bib">BibTeX</a> /
					<a href="#" onclick="toggleSummary('freqdebias'); return false;">Summary</a>
					<p id="freqdebias_summary" align="justify" style="display:none;">
						We introduce a novel frequency-domain debiasing framework that uses consistency regularization to improve the generalization of deepfake detectors to unseen manipulations.
					</p>
				</td>
			</tr>
			<tr onmouseout="tcd_stop_DiSa()" onmouseover="tcd_start_DiSa()">
				<td style="padding:10px;width:25%;vertical-align:top">
				  <div class="image-container">
					<img id="paper_DiSa" src="Doc/DiSa_overview.png" alt="Image of DiSa Paper" width="200" style="border-style: none">
				  </div>

				  <script type="text/javascript">
					// Function to change the image when hovered (DiSa)
					function tcd_start_DiSa() {
					  document.getElementById('paper_DiSa').src = "Doc/DiSa_overview.png"; // Change to a different image if needed
					}

					// Function to revert back to the original image (DiSa)
					function tcd_stop_DiSa() {
					  document.getElementById('paper_DiSa').src = "Doc/DiSa_overview.png"; // Change to a different image if needed
					}
				  </script>
				</td>
				<td style="padding-bottom:35px;width:75%;vertical-align:middle">
					<a href="https://arxiv.org/abs/2505.19373">
					  <papertitle style="color:#1772d0;">DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models</papertitle>
					</a>
					<br>
					<strong>Niloufar Alipour Talemi, Hossein Kashiani, Hossein R Nowdeh, Fatemeh Afghah</strong>
					<br>
					<em>Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</em>, 2025.
					<br>
					<a href="https://dl.acm.org/doi/10.1145/3711896.3736911">ACM</a> /
					<a href="https://arxiv.org/abs/2505.19373">arxiv</a> /
					<a href="#" onclick="toggleSummary('disa'); return false;">Summary</a>
					<p id="disa_summary" align="justify" style="display:none;">
					  We propose DiSa, a directional saliency-aware prompt learning framework that improves generalization in vision-language models by integrating cross-interactive regularization and directional regularization strategies.
					</p>
				</td>
			</tr>
			<tr onmouseout="tcd_stop_stylepro()" onmouseover="tcd_start_stylepro()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="paper_stylepro" src="Doc/stylepro.png" alt="Image of Style-Pro Paper" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered (Style-Pro)
                  function tcd_start_stylepro() {
                   document.getElementById('paper_stylepro').src = "Doc/stylepro.png"; // Change to a different image if needed
                  }

                  // Function to revert back to the original image (Style-Pro)
                  function tcd_stop_stylepro() {
                   document.getElementById('paper_stylepro').src = "Doc/stylepro.png"; // Change to a different image if needed
                  }
                </script>
               </td>

               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2411.16018" id="StylePro_journal">
                  <papertitle style="color:#1772d0;">Style-Pro: Style-Guided Prompt Learning for Generalizable Vision-Language Models.</papertitle>
                </a>
                <br>
                <strong>NA Talemi, H Kashiani, F Afghah</strong>
                <br>
                <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2025.
                <br>
                <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Talemi_Style-Pro_Style-Guided_Prompt_Learning_for_Generalizable_Vision-Language_Models_WACV_2025_paper.pdf">CVF</a> /
                <a href="https://arxiv.org/abs/2411.16018">arxiv</a> /
                <a href="Doc/Poster_StylePro_WACV2025.pdf">Poster</a> /
                <a href="Doc/Style-Pro">BibTeX</a> /
                <a href="#" onclick="toggleSummary('stylepro'); return false;">Summary</a>
                <p id="stylepro_summary" align="justify" style="display:none;">
                  We propose a style-guided prompt learning framework to enhance generalization in Vision-Language models.
                </p>
               </td>
             </tr>


            <tr onmouseout="tcd_stop_roads()" onmouseover="tcd_start_roads()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="paper_an" src="Doc/anomaly_1.png" alt="Image of ROADS Paper" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start_roads() {
                   document.getElementById('paper_an').src = "Doc/anomaly_2.jpg";
                  }

                  // Function to revert back to the original image
                  function tcd_stop_roads() {
                   document.getElementById('paper_an').src = "Doc/anomaly_1.png";
                  }
                </script>
               </td>

               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2411.16049" id="ROADS_journal">
                  <papertitle style="color:#1772d0;">ROADS: Robust Prompt-driven Multi-Class Anomaly Detection under Domain Shift.</papertitle>
                </a>
                <br>
                <strong>H Kashiani, NA Talemi, F Afghah</strong>
                <br>
                <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2025.
                <br>
                <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Kashiani_ROADS_Robust_Prompt-Driven_Multi-Class_Anomaly_Detection_under_Domain_Shift_WACV_2025_paper.pdf">CVF</a> /
                <a href="https://arxiv.org/abs/2411.16049">arxiv</a> /
                <a href="Doc/Poster_ROADS_WACV2025.pdf">Poster</a> /
                <a href="https://www.youtube.com/watch?v=A6tYQUHMyno">Video (Best WACV)</a> /
                <a href="Doc/ROADS">BibTeX</a> /
                <a href="#" onclick="toggleSummary('roads'); return false;">Summary</a>
                <p id="roads_summary" align="justify" style="display:none;">
                   We propose a robust multi-class anomaly detection framework with a class-aware prompt integration mechanism to mitigate inter-class interference and a domain adapter to handle domain shifts.
                </p>
               </td>
             </tr>


             <tr onmouseout="tcd_stop()" onmouseover="tcd_start()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="paper-img" src="Doc/CATFACE_1.png" alt="CATFace" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start() {
                   document.getElementById('paper-img').src = "Doc/CATFACE__2.png";
                  }

                  // Function to revert back to the original image
                  function tcd_stop() {
                   document.getElementById('paper-img').src = "Doc/CATFACE_1.png";
                  }
                </script>
               </td>

            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
             <a href="https://arxiv.org/pdf/2401.03037" id="CATFace_journal">
               <papertitle style="color:#1772d0;">CATFace: Cross-Attribute-Guided Transformer With Self-Attention Distillation for Low-Quality Face Recognition.</papertitle>
             </a>
             <br>
             <strong>NA Talemi, H Kashiani, NM Nasrabadi</strong>
             <br>
             <em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>, 2024.
             <br>
             <a href="https://arxiv.org/pdf/2401.03037">arxiv</a> /
             <a href="https://ieeexplore.ieee.org/abstract/document/10380201/">IEEE</a> /
             <a href="Doc/catface.bib">BibTeX</a> /
             <a href="#" onclick="toggleSummary('catface'); return false;">Summary</a>
             <p id="catface_summary" align="justify" style="display:none;">
               We propose a novel multi-branch network with cross-attribute-guided fusion and self-attention distillation, improving face recognition in low-quality images using soft biometric attributes.
             </p>
            </td>
          </tr>


                    <tr onmouseout="tcd_stop_medvit()" onmouseover="tcd_start_medvit()">
            <td style="padding:10px;width:25%;vertical-align:top">
             <div class="image-container">
               <img id="medvit-img" src="Doc/MedViT.png" alt="MedViT" width="200" style="border-style: none">
             </div>

             <script type="text/javascript">
               // Function to change the image when hovered
               function tcd_start_medvit() {
                document.getElementById('medvit-img').src = "Doc/MedViT_2_edit.png";
               }

               // Function to revert back to the original image
               function tcd_stop_medvit() {
                document.getElementById('medvit-img').src = "Doc/MedViT.png";
               }
             </script>
            </td>

            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
             <a href="https://arxiv.org/pdf/2302.09462" id="MedViT_journal">
               <papertitle style="color:#1772d0;">MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.</papertitle>
             </a>
             <br>
             <strong>ON Manzari, H Ahmadabadi, H Kashiani, SB Shokouhi, A Ayatollahi</strong>
             <br>
             <em>Computers in Biology and Medicine</em>, 2023.
             <br>
             <a href="https://arxiv.org/pdf/2302.09462">arxiv</a> /
             <a href="https://www.sciencedirect.com/science/article/pii/S0010482523002561">Elsevier</a> /
             <a href="Doc/medvit.bib">BibTeX</a> /
             <a href="https://github.com/Omid-Nejati/MedViT">code</a> /
             <a href="#" onclick="toggleSummary('medvit'); return false;">Summary</a>
             <p id="medvit_summary" align="justify" style="display:none;">
               This study proposes a robust and efficient CNN-Transformer hybrid model, combining CNN locality with the global connectivity of vision Transformers. Additionally, we enhance robustness by learning smoother decision boundaries through feature mean and variance permutation within mini-batches.
             </p>
            </td>
          </tr>
                              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="Doc/AAFACE.png" alt="AAFACE" width="200" style="border-style: none">
                </td>
                <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2308.07243" id="AAFACE_journal">
                    <papertitle style="color:#1772d0;">AAFACE: Attribute-aware Attentional Network for Face Recognition.</papertitle>
                  </a>
                  <br>
                  <strong>NA Talemi, H Kashiani, and 4 more authors</strong>
                  <br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2023.
                  <br>
                  <a href="https://arxiv.org/pdf/2308.07243">arxiv</a> /
                  <a href="https://ieeexplore.ieee.org/abstract/document/10222666">IEEE</a> /
                  <a href="Doc/AAFACE.bib">BibTeX</a> /
                  <a href="#" onclick="toggleSummary('aaface'); return false;">Summary</a>
                  <p id="aaface_summary" align="justify" style="display:none;">We present a multi-branch network using attribute-aware integration to enhance face recognition through soft biometric prediction.
                    </p>
                </td>
              </tr>


          <tr onmouseout="tcd_stop_morph()" onmouseover="tcd_start_morph()">
            <td style="padding:10px;width:25%;vertical-align:top">
             <div class="image-container">
               <img id="morph-img" src="Doc/MorphAttackDetection.png" alt="Morph Attack Detection" width="200" style="border-style: none">
             </div>

             <script type="text/javascript">
               // Function to change the image when hovered
               function tcd_start_morph() {
                document.getElementById('morph-img').src = "Doc/MorphAttackDetection_2.png";
               }

               // Function to revert back to the original image
               function tcd_stop_morph() {
                document.getElementById('morph-img').src = "Doc/MorphAttackDetection.png";
               }
             </script>
            </td>

            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
             <a href="https://arxiv.org/pdf/2308.10392" id="MorphDetection_journal">
               <papertitle style="color:#1772d0;">Towards Generalizable Morph Attack Detection with Consistency Regularization.</papertitle>
             </a>
             <br>
             <strong>H Kashiani, NA Talemi, NM Nasrabadi</strong>
             <br>
             <em>IEEE International Joint Conference on Biometrics (IJCB)</em>, 2023.
             <br>
             <a href="https://arxiv.org/pdf/2308.10392">arxiv</a> /
             <a href="https://ieeexplore.ieee.org/abstract/document/10448876/">IEEE</a> /
             <a href="data/morph.bib">BibTeX</a> /
             <a href="Doc/Poster-IJCB2023.pdf">Poster</a> /
             <a href="https://github.com/kashiani/SelfMorphing_GRL">code</a> /
             <a href="#" onclick="toggleSummary('morphdetection'); return false;">Summary</a>
             <p id="morphdetection_summary" align="justify" style="display:none;">
               We propose consistency regularization to enhance the generalization of morph attack detection through morph-wise augmentations to enhance robustness against unseen morph attacks in biometric systems.
             </p>
            </td>
          </tr>


              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="Doc/FaceQualityVector.png" alt="Face Quality Vector" width="200" style="border-style: none">
                </td>
                <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/WACV2023W/WVAQ/papers/Najafzadeh_Face_Image_Quality_Vector_Assessment_for_Biometrics_Applications_WACVW_2023_paper.pdf" id="FaceQuality_journal">
                    <papertitle style="color:#1772d0;">Face Image Quality Vector Assessment for Biometrics Applications.</papertitle>
                  </a>
                  <br>
                  <strong>N Najafzadeh, H Kashiani, and 4 more authors</strong>
                  <br>
                  <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023.
                  <br>
                  <a href="https://openaccess.thecvf.com/content/WACV2023W/WVAQ/papers/Najafzadeh_Face_Image_Quality_Vector_Assessment_for_Biometrics_Applications_WACVW_2023_paper.pdf">CVF</a> /
                  <a href="https://ieeexplore.ieee.org/document/10031228">IEEE</a> /
                  <a href="data/facequality.bib">BibTeX</a> /
                  <a href="#" onclick="toggleSummary('facequality'); return false;">Summary</a>
                  <p id="facequality_summary" align="justify" style="display:none;">This paper proposes a multi-task neural network that generates a face quality vector, including nuisance factors, offering improved performance and detailed feedback for face image quality assessment.</p>
                </td>
              </tr>


                                  <tr onmouseout="tcd_stop_ensemble()" onmouseover="tcd_start_ensemble()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="ensemble-img" src="Doc/RobustEnsemble_2_edit.png" alt="Robust Ensemble" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start_ensemble() {
                   document.getElementById('ensemble-img').src = "Doc/RobustEnsemble.png";
                  }

                  // Function to revert back to the original image
                  function tcd_stop_ensemble() {
                   document.getElementById('ensemble-img').src = "Doc/RobustEnsemble_2_edit.png";
                  }
                </script>
               </td>

               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2209.08130" id="EnsembleMorph_journal">
                  <papertitle style="color:#1772d0;">Robust Ensemble Morph Detection with Domain Generalization.</papertitle>
                </a>
                <br>
                <strong>H Kashiani, SM Sami, S Soleymani, NM Nasrabadi</strong>
                <br>
                <em>IEEE International Joint Conference on Biometrics (IJCB)</em>, 2022.
                <br>
                <a href="https://arxiv.org/pdf/2209.08130">arxiv</a> /
                <a href="https://ieeexplore.ieee.org/abstract/document/10007929/">IEEE</a> /
                <a href="data/morphensemble.bib">BibTeX</a> /
                <a href="Doc/Poster-IJCB2022.pdf">Poster</a> /
                <a href="Doc/Video-IJCB2022.mp4">Video</a> /
                <a href="https://pages.nist.gov/frvt/reports/morph/frvt_morph_report.pdf">FRVT Results</a> /
                <a href="https://github.com/kashiani/Face-Morphing-Attack-Detection-Benchmark">code</a> /
                <a href="#" onclick="toggleSummary('morphensemble'); return false;">Summary</a>
                <p id="morphensemble_summary" align="justify" style="display:none;">
                  This paper proposes a robust ensemble of CNNs and Transformers for morph detection that enhances generalization to morph attacks and increases robustness against adversarial threats through multi-perturbation training.
                </p>
               </td>
             </tr>

             <tr onmouseout="tcd_stop_transformer()" onmouseover="tcd_start_transformer()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="transformer-img" src="Doc/Robust_transformer_2.png" alt="Robust Transformer" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start_transformer() {
                   document.getElementById('transformer-img').src = "Doc/Robust_transformer.png";
                  }

                  // Function to revert back to the original image
                  function tcd_stop_transformer() {
                   document.getElementById('transformer-img').src = "Doc/Robust_transformer_2.png";
                  }
                </script>
               </td>





               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2301.11553" id="Transformer_journal">
                  <papertitle style="color:#1772d0;">Robust Transformer with Locality Inductive Bias and Feature Normalization.</papertitle>
                </a>
                <br>
                <strong>ON Manzari, H Kashiani, HA Dehkordi, SB Shokouhi</strong>
                <br>
                <em>Engineering Science and Technology</em>, 2022.
                <br>
                <a href="https://arxiv.org/pdf/2301.11553">arxiv</a> /
                <a href="https://www.sciencedirect.com/science/article/pii/S2215098622002294">Elsevier</a> /
                <a href="data/robust_transformer.bib">BibTeX</a> /
                <a href="https://github.com/Omid-Nejati/Locality-iN-Locality">code</a> /
                <a href="#" onclick="toggleSummary('robusttransformer'); return false;">Summary</a>
                <p id="robusttransformer_summary" align="justify" style="display:none;">
                  This paper proposes a robust transformer model that incorporates locality inductive bias and feature normalization, enhancing generalization and robustness in feature extraction tasks.
                </p>
               </td>
             </tr>

             <tr onmouseout="tcd_stop_humanaction()" onmouseover="tcd_start_humanaction()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="humanaction-img" src="Doc/HumanActionRecognition_2.png" alt="Human Action Recognition" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start_humanaction() {
                   document.getElementById('humanaction-img').src = "Doc/HumanActionRecognition.png";
                  }

                  // Function to revert back to the original image
                  function tcd_stop_humanaction() {
                   document.getElementById('humanaction-img').src = "Doc/HumanActionRecognition_2.png";
                  }
                </script>
               </td>

               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2112.07015" id="HumanAction_journal">
                  <papertitle style="color:#1772d0;">Multi-expert Human Action Recognition with Hierarchical Super-class Learning.</papertitle>
                </a>
                <br>
                <strong>HA Dehkordi, AS Nezhad, H Kashiani, SB Shokouhi, A Ayatollahi</strong>
                <br>
                <em>Knowledge-Based Systems</em>, 2022.
                <br>
                <a href="https://arxiv.org/pdf/2112.07015">arxiv</a> /
                <a href="https://www.sciencedirect.com/science/article/pii/S0950705122005378">Elsevier</a> /
                <a href="data/humanaction.bib">BibTeX</a> /
                <a href="#" onclick="toggleSummary('humanaction'); return false;">Summary</a>
                <p id="humanaction_summary" align="justify" style="display:none;">
                  We propose a two-phase multi-expert classification method for human action recognition, addressing long-tailed distribution using super-class learning without extra data or manual annotation. A novel Graph-Based Class Selection (GCS) algorithm optimizes class configurations and inter-class dependencies.
                </p>
               </td>
             </tr>

             <tr onmouseout="tcd_stop_generalizing()" onmouseover="tcd_start_generalizing()">
               <td style="padding:10px;width:25%;vertical-align:top">
                <div class="image-container">
                  <img id="generalizing-img" src="Doc/Generalizing_1.png" alt="Autonomous Vehicles" width="200" style="border-style: none">
                </div>

                <script type="text/javascript">
                  // Function to change the image when hovered
                  function tcd_start_generalizing() {
                   document.getElementById('generalizing-img').src = "Doc/Generalizing_2.png";
                  }

                  // Function to revert back to the original image
                  function tcd_stop_generalizing() {
                   document.getElementById('generalizing-img').src = "Doc/Generalizing_1.png";
                  }
                </script>
               </td>

               <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0957417421008368" id="AutonomousVehicles_journal">
                  <papertitle style="color:#1772d0;">Generalizing State-of-the-art Object Detectors for Autonomous Vehicles in Unseen Environments.</papertitle>
                </a>
                <br>
                <strong>A Khosravian, A Amirkhani, H Kashiani, M Masih-Tehrani</strong>
                <br>
                <em>Expert Systems with Applications</em>, 2021.
                <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0957417421008368">Elsevier</a> /
                <a href="data/autonomous.bib">BibTeX</a> /
                <a href="#" onclick="toggleSummary('autonomous'); return false;">Summary</a>
                <p id="autonomous_summary" align="justify" style="display:none;">
                  We address the generalization issues in scene understanding for autonomous vehicles by employing GANs for weather modeling, and advanced augmentations, improving object detection robustness and generalization across domains, especially in adverse weather conditions and natural distortions.
                </p>
               </td>
             </tr>

              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="Doc/COVIDDetection.png" alt="COVID Detection" width="200" style="border-style: none">
                </td>
                <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                  <a href="https://www.researchgate.net/profile/Hojat-Asgarian-Dehkordi/publication/358999207_Lightweight_Local_Transformer_for_COVID-19_Detection_Using_Chest_CT_Scans/links/624fe178d726197cfd45233b/Lightweight-Local-Transformer-for-COVID-19-Detection-Using-Chest-CT-Scans.pdf" id="COVIDDetection_journal">
                    <papertitle style="color:#1772d0;">Lightweight Local Transformer for COVID-19 Detection Using Chest CT Scans.</papertitle>
                  </a>
                  <br>
                  <strong>HA Dehkordi, H Kashiani, AAH Imani, SB Shokouhi</strong>
                  <br>
                  <em>International Conference on Computer Engineering and Knowledge</em>, 2021.
                  <br>
                  <a href="https://www.researchgate.net/profile/Hojat-Asgarian-Dehkordi/publication/358999207_Lightweight_Local_Transformer_for_COVID-19_Detection_Using_Chest_CT_Scans/links/624fe178d726197cfd45233b/Lightweight-Local-Transformer-for-COVID-19-Detection-Using-Chest-CT-Scans.pdf">arxiv</a> /
                  <a href="https://ieeexplore.ieee.org/abstract/document/9721517/">IEEE</a> /
                  <a href="data/covid.bib">BibTeX</a> /
                <a href="https://www.youtube.com/watch?v=1Xv-gX8jiRU">Video</a> /
                <a href="#" onclick="toggleSummary('covid'); return false;">Summary</a>
                  <p id="covid_summary" align="justify" style="display:none;">This paper introduces a hybrid CNN-Transformer model for COVID-19 diagnosis using CT images, combining local and global feature extraction, and achieving superior performance with limited training data.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="Doc/SemanticSegmentation.png" alt="Semantic Segmentation" width="200" style="border-style: none">
                </td>
                <td style="padding-bottom:35px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/iel7/6287639/9312710/09522137.pdf" id="SemanticSegmentation_journal">
                    <papertitle style="color:#1772d0;">Robust Semantic Segmentation with Multi-Teacher Knowledge Distillation.</papertitle>
                  </a>
                  <br>
                  <strong>A Amirkhani, A Khosravian, M Masih-Tehrani, H Kashiani</strong>
                  <br>
                  <em>IEEE Access</em>, 2021.
                  <br>
                  <a href="https://ieeexplore.ieee.org/iel7/6287639/9312710/09522137.pdf">IEEE</a> /
                  <a href="data/semanticseg.bib">BibTeX</a> /
                  <a href="#" onclick="toggleSummary('semanticseg'); return false;">Summary</a>
                  <p id="semanticseg_summary" align="justify" style="display:none;">We propose a multi-teacher KD framework in which several expert CNNs, trained on different settings, supervise a lightweight student model. This framework enhances the robustness and performance of the student by using diverse knowledge sources.</p>
                </td>
              </tr>

          <tr onmouseout="imavis_stop()" onmouseover="imavis_start()">
            <td style="padding:10px;padding-top:40px;width:25%;vertical-align:middle; position: relative;">
             <div class="one" style="position: relative; width: 200px; height: auto;">
               <div class="two" id="imavis_video" style="position: absolute; top; left: 0; visibility: hidden; width: 200px; height: auto;">
                <video width="200" style="border-style: none" muted autoplay loop>
                  <source src="Doc/IMAVIS_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
               </div>
               <img src="Doc/IMAVIS_1.png" alt="Visual Object Tracking" width="200" style="border-style: none">
             </div>

             <script type="text/javascript">
               function imavis_start() {
                document.getElementById('imavis_video').style.visibility = "visible";
               }

               function imavis_stop() {
                document.getElementById('imavis_video').style.visibility = "hidden";
               }

               // Initialize with the video hidden
               imavis_stop();
             </script>
            </td>

            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
             <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300113" id="Tracking_journal">
               <papertitle style="color:#1772d0;">Visual Object Tracking Based on Adaptive Siamese and Motion Estimation Network.</papertitle>
             </a>
             <br>
             <strong>Hossein Kashiani, Shahriar B Shokouhi</strong>
             <br>
             <em>Image and Vision Computing</em>, 2019.
             <br>
             <a href="https://arxiv.org/pdf/1810.00119">arxiv</a> /
             <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300113">Elsevier</a> /
             <a href="data/tracking.bib">BibTeX</a> /
             <a href="#" onclick="toggleSummary('tracking'); return false;">Summary</a>
             <p id="tracking_summary" align="justify" style="display:none;">
               This work aims to improve motion and observation models in visual object tracking. We propose a motion estimation network to refine target location predictions, with a Siamese network detecting the most probable candidate. Additionally, a weighting CNN adaptively assigns weights to similarity scores, accounting for target appearance changes.
             </p>
            </td>
          </tr>


          <tr onmouseout="iccke_stop()" onmouseover="iccke_start()">
            <td style="padding:10px;padding-top:40px;width:25%;vertical-align:middle; position: relative;">
             <div class="one" style="position: relative; width: 200px; height: auto;">
               <div class="two" id="iccke_gif" style="position: absolute; top: 0; left: 0; visibility: hidden; width: 200px; height: auto;">
                <img src="Doc/ICCKE_2.gif" alt="Patchwise Object Tracking GIF" width="200" style="border-style: none">
               </div>
               <img src="Doc/ICCKE_1.png" alt="Patchwise Object Tracking" width="200" style="border-style: none">
             </div>

             <script type="text/javascript">
               function iccke_start() {
                document.getElementById('iccke_gif').style.visibility = "visible";
               }

               function iccke_stop() {
                document.getElementById('iccke_gif').style.visibility = "hidden";
               }

               // Initialize with the GIF hidden
               iccke_stop();
             </script>
            </td>

            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
             <a href="https://ieeexplore.ieee.org/document/8167940" id="PatchwiseTracking_journal">
               <papertitle style="color:#1772d0;">Patchwise Object Tracking via Structural Local Sparse Appearance Model.</papertitle>
             </a>
             <br>
             <strong>Hossein Kashiani, Shahriar B Shokouhi</strong>
             <br>
             <em>International Conference on Computer and Knowledge Engineering</em>, 2017.
             <br>
             <a href="https://arxiv.org/pdf/1803.06141">arxiv</a> /
             <a href="https://ieeexplore.ieee.org/document/8167940">IEEE</a> /
             <a href="data/patchwise.bib">BibTeX</a> /
             <a href="#" onclick="toggleSummary('patchwise'); return false;">Summary</a>
             <p id="patchwise_summary" align="justify" style="display:none;">This paper proposes a robust tracking method that exploits relationships between target patches in adjacent frames using a sparse appearance model.</p>
            </td>
          </tr>

              </tbody>
            </table>
          </div>

          <table id="talks" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Talks</heading>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:top">
                  <table style="width:130%;max-width:1040px;margin:0 auto;border:0px;border-spacing:15px;border-collapse:separate;">
                    <tbody>
                      <tr>
                        <!-- First Talk -->
                        <td style="width:50%;vertical-align:top;padding:10px;">
                          <papertitle style="display:block;text-align:center;margin-bottom:15px;">Best of WACV 2025</papertitle>
                          <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;width:100%;">
                            <iframe
                              style="position:absolute;top:0;left:0;width:100%;height:100%;"
                              src="https://www.youtube.com/embed/A6tYQUHMyno"
                              title="Best of WACV 2025"
                              frameborder="0"
                              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                              allowfullscreen>
                            </iframe>
                          </div>
                        </td>
                        <!-- Second Talk -->
                        <td style="width:50%;vertical-align:top;padding:10px;">
                          <papertitle style="display:block;text-align:center;margin-bottom:15px;">Face Morphing Attack Detection</papertitle>
                          <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;width:100%;">
                            <iframe
                              style="position:absolute;top:0;left:0;width:100%;height:100%;"
                              src="https://www.youtube.com/embed/AlGhfrbxQlY"
                              title="Face Morphing Attack Detection"
                              frameborder="0"
                              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                              allowfullscreen>
                            </iframe>
                          </div>
                        </td>
                      </tr>
                      <tr>
                        <!-- Third Talk - Centered -->
                        <td style="width:100%;vertical-align:top;padding:10px;" colspan="2">
                          <table style="width:50%;max-width:520px;margin:0 auto;border:0px;">
                            <tbody>
                              <tr>
                                <td style="width:100%;vertical-align:top;padding:10px;">
                                  <papertitle style="display:block;text-align:center;margin-bottom:15px;">Unlocking Visual Anomaly Detection: Navigating Challenges and Pioneering with Vision-Language Models</papertitle>
                                  <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;width:100%;">
                                    <iframe
                                      style="position:absolute;top:0;left:0;width:100%;height:100%;"
                                      src="https://www.youtube.com/embed/AfA0dtTY6_o"
                                      title="Unlocking Visual Anomaly Detection: Navigating Challenges and Pioneering with Vision-Language Models"
                                      frameborder="0"
                                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                      allowfullscreen>
                                    </iframe>
                                  </div>
                                  <p style="text-align:center;margin-top:15px;">
                                    <a href="Doc/Presentation.pdf" target="_blank">Slides</a>
                                  </p>
                                </td>
                              </tr>
                            </tbody>
                          </table>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </td>
              </tr>
            </tbody>
          </table>

          <table id="academic-service" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
             <heading>Academic Service</heading>
             <ul>
               <li><b>Reviewer</b>: Prestigious academic venues in computer vision, machine learning, and artificial intelligence, including:</li>
               <ul>
                <li><a href="https://aaai.org/conference/aaai/aaai-25/" target="_blank">AAAI Conference on Artificial Intelligence (AAAI) (2024, 2025)</a>.</li>
                <li><a href="https://cvpr.thecvf.com/" target="_blank">IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) (2024, 2025)</a>.</li>
                <li><a href="https://2024.ieeeicme.org/" target="_blank">International Conference on Multimedia and Expo (ICME) (2022, 2023, 2024)</a>.</li>
                <li><a href="https://2022.ieeeicip.org/" target="_blank">International Conference on Image Processing (ICIP) (2021, 2022)</a>.</li>
                <li><a href="https://ieeeaccess.ieee.org/" target="_blank">IEEE Access</a>.</li>
                <li><a href="https://ieee-cas.org/publication/tcsvt" target="_blank">Transactions on Circuits and Systems for Video Technology (TCSVT)</a>, IEEE.</li>
                <li><a href="https://www.sciencedirect.com/journal/neurocomputing" target="_blank">Neurocomputing</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">Pattern Recognition</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/expert-systems-with-applications" target="_blank">Expert Systems with Applications</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/knowledge-based-systems" target="_blank">Knowledge-Based Systems</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/engineering-applications-of-artificial-intelligence" target="_blank">Engineering Applications of Artificial Intelligence</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/computers-in-biology-and-medicine" target="_blank">Computers in Biology and Medicine</a>, Elsevier.</li>
                <li><a href="https://www.sciencedirect.com/journal/engineering-science-and-technology-an-international-journal" target="_blank">International Journal of Engineering Science and Technology</a>, Elsevier.</li>
               </ul>
               <li>Review records available at <a href="https.webofscience.com/wos/author/record/ABF-6491-2020" target="_blank">Web of Science</a>.</li>
             </ul>
            </td>
          </tr>
         </tbody>
       </table>
      </tr>
        </td>






    </tbody>
  </table>
  </div>

<script src="particles.js"></script>

<script>
/* global particlesJS */

var particleSettings = {
  "particles": {
    "number": {
      "value": 50, // From your config
      "density": {
        "enable": true,
        "value_area": 220 // From your config
      }
    },
    "color": {
      "value": "#27ae60" // Default, will be overridden by randomColor
    },
    "shape": {
      "type": "circle",
      "stroke": {
        "width": 0,
        "color": "#27ae60"
      },
      "polygon": {
        "nb_sides": 5
      },
      "image": {
        "src": "img/github.svg",
        "width": 100,
        "height": 100
      }
    },
    "opacity": {
      "value": 0.5,
      "random": false,
      "anim": {
        "enable": false,
        "speed": 1,
        "opacity_min": 0.1,
        "sync": false
      }
    },
    "size": {
      "value": 3,
      "random": true,
      "anim": {
        "enable": false,
        "speed": 2,
        "size_min": 0.1,
        "sync": false
      }
    },
    "line_linked": {
      "enable": true,
      "distance": 150,
      "color": "#8e8e8e",
      "opacity": 0.4,
      "width": 1 // From your config
    },
    "move": {
      "enable": true,
      "speed": 5, // Increased from 2
      "direction": "none",
      "random": false,
      "straight": false,
      "out_mode": "out",
      "bounce": false,
      "attract": {
        "enable": false,
        "rotateX": 600,
        "rotateY": 1200
      }
    }
  },
  "interactivity": {
    "detect_on": "canvas",
    "events": {
      "onhover": {
        "enable": true,
        "mode": "repulse"
      },
      "onclick": {
        "enable": true,
        "mode": "push"
      },
      "resize": true
    },
    "modes": {
      "grab": {
        "distance": 400,
        "line_linked": {
          "opacity": 1
        }
      },
      "bubble": {
        "distance": 400,
        "size": 40,
        "duration": 2,
        "opacity": 8,
        "speed": 3
      },
      "repulse": {
        "distance": 100,
        "duration": 0.4
      },
      "push": {
        "particles_nb": 4
      },
      "remove": {
        "particles_nb": 2
      }
    }
  },
  "retina_detect": true
};

// Define color options
var colors = {
  'green': '#1772d0'
};

// Select a random color
    var randomColor = colors[Object.keys(colors)[Math.floor(Math.random() * Object.keys(colors).length)]];
    particleSettings.particles.color.value = randomColor;

    // Initialize particles with the settings
    particlesJS("particles-js", particleSettings);

    // Update link colors to match particle color
    document.addEventListener("DOMContentLoaded", function() {
      var elements = document.querySelectorAll('a');
      [].forEach.call(elements, function(value) {
        value.style.color = randomColor;
      });
    });

    window.onscroll = function() {
       let scrollY = window.scrollY || document.documentElement.scrollTop;
       if (scrollY > 1) {
        // Also hide the new fade div on scroll
        var particlesJsDiv = document.getElementById('particles-js');
        var centerFadeDiv = document.getElementById('center-fade');
        if (particlesJsDiv) particlesJsDiv.style.display = 'none';
        if (centerFadeDiv) centerFadeDiv.style.display = 'none';
       }
    };

    // Toggle summary visibility
    function toggleSummary(paperId) {
      const summaryElement = document.getElementById(paperId + '_summary');
      if (summaryElement) {
        if (summaryElement.style.display === 'none' || summaryElement.style.display === '') {
          summaryElement.style.display = 'block';
        } else {
          summaryElement.style.display = 'none';
        }
      }
    }

    // Toggle research details visibility
    function toggleResearchDetails() {
      const detailsElement = document.getElementById('research_details');
      if (detailsElement) {
        if (detailsElement.style.display === 'none' || detailsElement.style.display === '') {
          detailsElement.style.display = 'block';
        } else {
          detailsElement.style.display = 'none';
        }
      }
    }
  </script>
</body>
</html>



