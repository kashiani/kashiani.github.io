@article{MANZARI2023106791,
title = {MedViT: A robust vision transformer for generalized medical image classification},
journal = {Computers in Biology and Medicine},
volume = {157},
pages = {106791},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106791},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523002561},
author = {Omid Nejati Manzari and Hamid Ahmadabadi and Hossein Kashiani and Shahriar B. Shokouhi and Ahmad Ayatollahi},
keywords = {Medical image classification, Adversarial attack, Adversarial robustness, Vision transformer},
abstract = {Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, there are still concerns about the reliability of deep medical diagnosis systems against the potential threats of adversarial attacks since inaccurate diagnosis could lead to disastrous consequences in the safety realm. In this study, we propose a highly robust yet efficient CNN-Transformer hybrid model which is equipped with the locality of CNNs as well as the global connectivity of vision Transformers. To mitigate the high quadratic complexity of the self-attention mechanism while jointly attending to information in various representation subspaces, we construct our attention mechanism by means of an efficient convolution operation. Moreover, to alleviate the fragility of our Transformer model against adversarial attacks, we attempt to learn smoother decision boundaries. To this end, we augment the shape information of an image in the high-level feature space by permuting the feature mean and variance within mini-batches. With less computational complexity, our proposed hybrid model demonstrates its high robustness and generalization ability compared to the state-of-the-art studies on a large-scale collection of standardized MedMNIST-2D datasets.}
}